#!/usr/bin/env python3
# -*- coding: utf-8 -*
import argparse
import glob
import os
print("Importing ROOT...")
import ROOT
from subprocess import Popen, PIPE
from decimal import Decimal
from tools.orchestrator import BabyMakerOrchestrator

class VBSOrchestrator(BabyMakerOrchestrator):
    def __init__(self, year, study_name, study_exe, input_files, xsecs_json, n_workers=8, 
                 is_data=False):
        self.output_dir = f"studies/{study_name}/output/{year}"
        self.is_data = is_data
        if year == "2016preVFP":
            self.lumi = 19.52
        elif year == "2016postVFP":
            self.lumi = 16.81
        elif year == "2017":
            self.lumi = 41.48
        elif year == "2018":
            self.lumi = 59.83
        else:
            raise ValueError("year must be 2016, 2017, or 2018")
        os.makedirs(self.output_dir, exist_ok=True)
        super().__init__(study_exe, input_files, xsecs_json=xsecs_json, n_workers=n_workers)

    def _get_file_info(self, file_name):
        file_info = {
            "is_signal": False,
            "n_events": 0
        }
        if "VBS" in file_name:
            file_info["is_signal"] = True
        if "Run201" in file_name:
            self.is_data = True
        # Get output info
        file_info["output_dir"] = self.output_dir
        file_info["output_name"] = file_name.split('/')[6].split("_Tune")[0]
        if not self.is_data:
            # Get # events
            f = ROOT.TFile(file_name)
            t = f.Get("Runs")
            h = ROOT.TH1F("h", "h", 1, 0, 1)
            t.Draw("0>>h", "genEventSumw", "goff")
            file_info["n_events"] = h.GetBinContent(1)
            f.Close()
        return file_info

    def _get_log_files(self, input_file):
        file_info = self._get_file_info(input_file)
        stdout_file = f"{self.output_dir}/{file_info['output_name']}.out"
        stderr_file = f"{self.output_dir}/{file_info['output_name']}.err"
        return stdout_file, stderr_file

    def _get_job(self, input_file):
        file_info = self._get_file_info(input_file)
        cmd = [self.executable]
        cmd.append(f"--input_ttree=Events")
        cmd.append(f"--output_dir={file_info['output_dir']}")
        cmd.append(f"--output_name={file_info['output_name']}")
        if file_info["is_signal"]:
            cmd.append("--is_signal")
        if self.is_data:
            cmd.append("--is_data")
        else:
            xsec = self.get_xsec(input_file)
            sf = xsec*1000*self.lumi/(file_info["n_events"])
            cmd.append(f"--scale_factor={Decimal.from_float(sf)}")
        cmd.append(input_file)
        return cmd

def run_test_jobs():
    dummy_dir = "/nfs-7/userdata/dummy/PhChangSkim/Tag"
    dummy_files = [f"{dummy_dir}/NanoSample{i:02d}_TuneCP5/merged/merged.root" for i in range(1, 11)]
    skims = [(2016, dummy_files), (2017, dummy_files), (2018, dummy_files)]
    for year, files in skims:
        print(f"Orchestrating {year} jobs...")
        orchestrator = VBSOrchestrator(
            year, 
            "dummy",       # study name
            "./bin/dummy", # study executable generated by: make study=[STUDY]
            files,
            xsecs_json="", # JSON with xsec->sample mapping; there is one in data/
            n_workers=8,
            is_data=True   # set this to True so it skips sf calculation
        )
        orchestrator.run()

if __name__ == "__main__":
    cli = argparse.ArgumentParser(description="Run a given study in parallel")
    cli.add_argument(
        "--study", type=str, required=True,
        help="Name of the study to run"
    )
    cli.add_argument(
        "--n_workers", type=int, default=4,
        help="Maximum number of worker processes"
    )
    cli.add_argument(
        "--debug", action="store_true",
        help="Run in debug mode"
    )
    cli.add_argument(
        "--nomake", action="store_true",
        help="Do not run make before running the study"
    )
    args = cli.parse_args()

    if not args.nomake:
        print(f"make study={args.study} clean")
        Popen(f"make study={args.study} clean".split(), stdout=PIPE).wait()
        print(f"make study={args.study}")
        stdout, stderr = Popen(f"make study={args.study}".split(), stdout=PIPE, stderr=PIPE).communicate()
        if stderr:
            raise RuntimeError(
                f"make --study={args.study} failed with the following error\n\n"
                + stderr.decode("utf-8")
            )

    # Check that the PWD is correct
    vbs_pwd = os.getenv("VBSPWD")
    if vbs_pwd == "":
        print(f"ERROR: `source setup.sh` must be run first")
        exit()
    elif os.getcwd() != vbs_pwd:
        print(f"ERROR: must be run within {vbs_pwd}")
        exit()

    # Run study
    skims = {
        "2016preVFP": glob.glob("/nfs-7/userdata/jguiang/VBSVHSkim_3l/v1/*UL16NanoAODAPVv9*/merged.root"),
        "2016postVFP": glob.glob("/nfs-7/userdata/jguiang/VBSVHSkim_3l/v1/*UL16NanoAODv9*/merged.root"),
        "2017": glob.glob("/nfs-7/userdata/jguiang/VBSVHSkim_3l/v1/*UL17NanoAODv9*/merged.root"),
        "2018": glob.glob("/nfs-7/userdata/jguiang/VBSVHSkim_3l/v1/*UL18NanoAODv9*/merged.root"),
    }
    for year, files in skims.items():
        print(f"Orchestrating {year} jobs...")
        orchestrator = VBSOrchestrator(
            year, 
            args.study, 
            f"./bin/{args.study}", 
            files, 
            xsecs_json="data/xsecs.json",
            n_workers=args.n_workers,
            is_data=False
        )
        orchestrator.run()
