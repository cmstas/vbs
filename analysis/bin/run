#!/usr/bin/env python3
# -*- coding: utf-8 -*
import argparse
import logging
import glob
import json
import os
import uproot
from subprocess import Popen, PIPE
from decimal import Decimal
from utils.orchestrator import Orchestrator

class VBSOrchestrator(Orchestrator):
    def __init__(self, study_name, study_exe, input_files, xsecs_json, n_workers=8):
        self.output_dir = f"studies/{study_name}/output"
        super().__init__(study_exe, input_files, xsecs_json=xsecs_json, n_workers=n_workers)

    def get_year(self, input_file):
        if "HIPM_UL2016" in input_file or "RunIISummer20UL16" in input_file or "UL16NanoAODAPVv9" in input_file:
            return "2016preVFP"
        elif "UL2016" in input_file or "RunIISummer20UL16" in input_file:
            return "2016postVFP"
        elif "UL2017" in input_file or "RunIISummer20UL17" in input_file:
            return "2017"
        elif "UL2018" in input_file or "RunIISummer20UL18" in input_file:
            return "2018"
        else:
            raise Exception(f"No year found in {input_file}")

    def get_lumi(self, input_file):
        year = self.get_year(input_file)
        if year == "2016preVFP":
            return 19.52
        elif year == "2016postVFP":
            return 16.81
        elif year == "2017":
            return 41.48
        elif year == "2018":
            return 59.83

    def get_file_info(self, input_file):
        file_info = {}
        file_info["n_events"] = 0
        file_info["is_data"] = ("Run201" in input_file)
        # Get output info
        file_info["output_dir"] = f"{self.output_dir}/{self.get_year(input_file)}"
        os.makedirs(file_info["output_dir"], exist_ok=True)
        if not file_info["is_data"]:
            file_info["output_name"] = input_file.split("_Tune")[0].split('/')[-1]
            # Get number of events
            with uproot.open(input_file) as f:
                t = f.get("Runs")
                file_info["n_events"] = t["genEventSumw"].array(library="np").sum()
        else:
            file_info["output_name"] = input_file.split("UL")[0][:-1].split('/')[-1]

        file_info["is_signal"] = ("VBSWH" in file_info["output_name"])
        return file_info

    def _get_log_files(self, input_file):
        file_info = self.get_file_info(input_file)
        stdout_file = f"{self.output_dir}/{file_info['output_name']}.out"
        stderr_file = f"{self.output_dir}/{file_info['output_name']}.err"
        return stdout_file, stderr_file

    def _get_job(self, input_file):
        file_info = self.get_file_info(input_file)
        with open("data/wjets_ht-binned_xsecs.json", "r") as f_in:
            wjets_xsecs = json.load(f_in)
        cmd = [self.executable]
        cmd.append(f"--input_ttree=Events")
        cmd.append(f"--output_dir={file_info['output_dir']}")
        cmd.append(f"--output_name={file_info['output_name']}")
        if file_info["is_signal"]:
            cmd.append("--is_signal")
        if file_info["is_data"]:
            cmd.append("--is_data")
        else:
            xsec = self.get_xsec(input_file)
            lumi = self.get_lumi(input_file)
            sf = xsec*1000*lumi/(file_info["n_events"])
            if "WminusH_HToBB_WToLNu_M-125" in file_info["output_name"]:
                sf *= 137.64/lumi # TODO: delete this when 2018 sample is not longer missing
            if "WJetsToLNu_HT-" in file_info["output_name"]:
                sf *= wjets_xsecs[self.get_year(input_file)][file_info["output_name"]]
            cmd.append(f"--scale_factor={Decimal.from_float(sf)}")
        cmd.append(input_file)
        return cmd

if __name__ == "__main__":
    cli = argparse.ArgumentParser(description="Run a given study in parallel")
    cli.add_argument(
        "study", type=str,
        help="Name of the study to run"
    )
    cli.add_argument(
        "--n_workers", type=int, default=8,
        help="Maximum number of worker processes"
    )
    cli.add_argument(
        "--nomake", action="store_true",
        help="Do not run make before running the study"
    )
    cli.add_argument(
        "--data", action="store_true",
        help="Run looper over data files (in addition to MC)"
    )
    args = cli.parse_args()

    os.makedirs(f"studies/{args.study}/output", exist_ok=True)
    logging.basicConfig(
        filename=f"studies/{args.study}/output/run.log",
        filemode="w",
        format="%(levelname)s [%(asctime)s]: %(message)s",
        datefmt="%m-%d-%Y %H:%M:%S %p",
        level="DEBUG"
    )

    if not args.nomake:
        print(f"make study={args.study} clean")
        Popen(f"make study={args.study} clean".split(), stdout=PIPE).wait()
        print(f"make study={args.study}")
        stdout, stderr = Popen(f"make study={args.study}".split(), stdout=PIPE, stderr=PIPE).communicate()
        if stderr:
            raise RuntimeError(
                f"make --study={args.study} failed with the following error\n\n"
                + stderr.decode("utf-8")
            )

    # Check that the PWD is correct
    vbs_pwd = os.getenv("VBSPWD")
    if vbs_pwd == "":
        print(f"ERROR: `source setup.sh` must be run first")
        exit()
    elif os.getcwd() != vbs_pwd:
        print(f"ERROR: must be run within {vbs_pwd}")
        exit()

    # Gather samples
    ceph_dir = "/ceph/cms/store/user/jguiang/VBSVHSkim"
    skim = "1lep_1ak8_2ak4_postskim"
    samples = []
    # Add data samples
    if args.data:
        samples += glob.glob(f"{ceph_dir}/data_{skim}/*HIPM_UL2016*NanoAODv9*/merged.root")
        samples += glob.glob(f"{ceph_dir}/data_{skim}/*UL2016*NanoAODv9*/merged.root")
        samples += glob.glob(f"{ceph_dir}/data_{skim}/*UL2017*NanoAODv9*/merged.root")
        samples += glob.glob(f"{ceph_dir}/data_{skim}/*UL2018*NanoAODv9*/merged.root")
    # Add background samples
    samples += glob.glob(f"{ceph_dir}/bkg_{skim}/*UL16NanoAODAPVv9*/merged.root")
    samples += glob.glob(f"{ceph_dir}/bkg_{skim}/*UL16NanoAODv9*/merged.root")
    samples += glob.glob(f"{ceph_dir}/bkg_{skim}/*UL17NanoAODv9*/merged.root")
    samples += glob.glob(f"{ceph_dir}/bkg_{skim}/*UL18NanoAODv9*/merged.root")
    # Add signal samples
    samples.append(f"{ceph_dir[:-10]}/VBSWHSignalGeneration/v1/VBSWH_mkW_Inclusive_TuneCP5_RunIISummer20UL16APV-106X_privateMC_NANOGEN_v1/merged.root")
    samples.append(f"{ceph_dir[:-10]}/VBSWHSignalGeneration/v1/VBSWH_mkW_Inclusive_TuneCP5_RunIISummer20UL16-106X_privateMC_NANOGEN_v1/merged.root")
    samples.append(f"{ceph_dir[:-10]}/VBSWHSignalGeneration/v1/VBSWH_mkW_Inclusive_TuneCP5_RunIISummer20UL16-106X_privateMC_NANOGEN_v1/merged.root")
    samples.append(f"{ceph_dir[:-10]}/VBSWHSignalGeneration/v1/VBSWH_mkW_Inclusive_TuneCP5_RunIISummer20UL18-106X_privateMC_NANOGEN_v1/merged.root")
    # samples += glob.glob(f"{ceph_dir}/sig_{skim}/VBSWH_mkW*RunIISummer20UL16APV*/*.root")
    # samples += glob.glob(f"{ceph_dir}/sig_{skim}/VBSWH_mkW*RunIISummer20UL16*/*.root")
    # samples += glob.glob(f"{ceph_dir}/sig_{skim}/VBSWH_mkW*RunIISummer20UL17*/*.root")
    # samples += glob.glob(f"{ceph_dir}/sig_{skim}/VBSWH_mkW*RunIISummer20UL18*/*.root")
    print(f"Orchestrating {len(samples)} jobs...")
    # Run study
    orchestrator = VBSOrchestrator(
        args.study, 
        f"./bin/{args.study}", 
        samples, 
        xsecs_json="data/xsecs.json",
        n_workers=args.n_workers
    )
    orchestrator.run()
