#!/usr/bin/env python3
# -*- coding: utf-8 -*
import argparse
import logging
import glob
import json
import os
import re
import uproot
from subprocess import Popen, PIPE
from decimal import Decimal
from utils.orchestrator import Orchestrator

class VBSOrchestrator(Orchestrator):
    def __init__(self, output_dir, output_ttree, study_exe, input_files, xsecs_json, 
                 variation="", n_workers=8):
        self.output_dir = output_dir
        self.output_ttree = output_ttree
        self.variation = variation
        super().__init__(study_exe, input_files, xsecs_json=xsecs_json, n_workers=n_workers)

    def get_year(self, input_file):
        if "HIPM_UL2016" in input_file or "UL16NanoAODAPVv9" in input_file or "RunIISummer20UL16APV" in input_file:
            return "2016preVFP"
        elif "UL2016" in input_file or "UL16NanoAODv9" in input_file or "RunIISummer20UL16" in input_file:
            return "2016postVFP"
        elif "UL2017" in input_file or "UL17NanoAODv9" in input_file or "RunIISummer20UL17" in input_file:
            return "2017"
        elif "UL2018" in input_file or "UL18NanoAODv9" in input_file or "RunIISummer20UL18" in input_file:
            return "2018"
        else:
            raise Exception(f"No year found in {input_file}")

    def get_lumi(self, input_file):
        year = self.get_year(input_file)
        if year == "2016preVFP":
            return 19.52
        elif year == "2016postVFP":
            return 16.81
        elif year == "2017":
            return 41.48
        elif year == "2018":
            return 59.83

    def get_file_info(self, input_file):
        file_info = {}
        file_info["n_events"] = 0
        file_info["is_data"] = ("Run201" in input_file)
        # Get output info
        file_info["output_dir"] = f"{self.output_dir}/{self.get_year(input_file)}"
        os.makedirs(file_info["output_dir"], exist_ok=True)
        if not file_info["is_data"]:
            file_info["output_name"] = input_file.split("_Tune")[0].split('/')[-1]
            # Get number of events
            with uproot.open(input_file) as f:
                t = f.get("Runs")
                file_info["n_events"] = t["genEventSumw"].array(library="np").sum()
        else:
            file_info["output_name"] = input_file.split("/")[-2].split("UL")[0][:-1].split('/')[-1]

        file_info["is_signal"] = ("VBSWH" in file_info["output_name"])
        return file_info

    def _get_log_files(self, input_file):
        file_info = self.get_file_info(input_file)
        stdout_file = f"{file_info['output_dir']}/{file_info['output_name']}.out"
        stderr_file = f"{file_info['output_dir']}/{file_info['output_name']}.err"
        return stdout_file, stderr_file

    def _get_job(self, input_file):
        file_info = self.get_file_info(input_file)
        cmd = [self.executable]
        cmd.append(f"--input_ttree=Events")
        cmd.append(f"--output_dir={file_info['output_dir']}")
        cmd.append(f"--output_name={file_info['output_name']}")
        cmd.append(f"--output_ttree={self.output_ttree}")
        cmd.append(f"--variation={self.variation}")
        if file_info["is_signal"]:
            cmd.append("--is_signal")
        if file_info["is_data"]:
            cmd.append("--is_data")
        else:
            xsec = self.get_xsec(input_file)
            lumi = self.get_lumi(input_file)
            sf = xsec*1000*lumi/(file_info["n_events"])
            if "WminusH_HToBB_WToLNu_M-125" in file_info["output_name"]:
                sf *= 137.64/lumi # TODO: delete this when 2018 sample is not longer missing
            if "WJetsToLNu_HT-" in file_info["output_name"]:
                with open("data/wjets_ht-binned_xsecs.json", "r") as f_in:
                    wjets_xsecs = json.load(f_in)
                sf *= wjets_xsecs[self.get_year(input_file)][file_info["output_name"]]
            cmd.append(f"--scale_factor={Decimal.from_float(sf)}")
        cmd.append(input_file)
        return cmd

if __name__ == "__main__":
    # Check that the PWD is correct
    vbs_pwd = os.getenv("VBSPWD")
    if vbs_pwd == "":
        print(f"ERROR: `source setup.sh` must be run first")
        exit()
    elif os.getcwd() != vbs_pwd:
        print(f"ERROR: must be run within {vbs_pwd}")
        exit()

    cli = argparse.ArgumentParser(description="Run a given study in parallel")
    cli.add_argument(
        "study", type=str,
        help="Name of the study to run"
    )
    cli.add_argument(
        "--skim", type=str, default="",
        help="Name of skim (i.e. bkg_{SKIM}, sig_{SKIM}, data_{SKIM})"
    )
    cli.add_argument(
        "--skims", type=str, nargs="*", default=[],
        help="Space-separated list of standalone skims"
    )
    cli.add_argument(
        "--tag", type=str, default="",
        help="Unique tag for output"
    )
    cli.add_argument(
        "--var", type=str, default="",
        help="Type of variation (e.g. 'up', 'down', 'nominal', ...)"
    )
    cli.add_argument(
        "--filter", type=str,
        help="Regex filter for excluding matching datasets"
    )
    cli.add_argument(
        "--output_ttree", type=str, default="tree",
        help="Name of output ttree"
    )
    cli.add_argument(
        "--n_workers", type=int, default=8,
        help="Maximum number of worker processes"
    )
    cli.add_argument(
        "--no_make", action="store_true",
        help="Do not run make before running the study"
    )
    cli.add_argument(
        "--data", action="store_true",
        help="Run looper over data files (in addition to MC)"
    )
    args = cli.parse_args()

    ceph_dir = "/ceph/cms/store/user/jguiang/VBSVHSkim"
    if args.skim:
        if args.data:
            skims = [f"{prefix}_{args.skim}" for prefix in ["bkg", "sig", "data"]]
        else:
            skims = [f"{prefix}_{args.skim}" for prefix in ["bkg", "sig"]]
    elif args.skims:
        skims = args.skims
    else:
        raise Exception("no skims provided")

    samples = []
    for skim in skims:
        if skim == "sig_{args.skim}":
            samples += glob.glob(f"{ceph_dir}/{skim}/*RunIISummer20UL16*/*.root") # Also picks up RunIISummer20UL16APV
            samples += glob.glob(f"{ceph_dir}/{skim}/*RunIISummer20UL17*/*.root")
            samples += glob.glob(f"{ceph_dir}/{skim}/*RunIISummer20UL18*/*.root")
        else:
            samples += glob.glob(f"{ceph_dir}/{skim}/*UL16NanoAODAPVv9*/*.root")
            samples += glob.glob(f"{ceph_dir}/{skim}/*UL16NanoAODv9*/*.root")
            samples += glob.glob(f"{ceph_dir}/{skim}/*UL17NanoAODv9*/*.root")
            samples += glob.glob(f"{ceph_dir}/{skim}/*UL18NanoAODv9*/*.root")
    if not samples:
        print("No samples found in these directories:")
        print("\n".join([f"{ceph_dir}/{skim}" for skim in skims]))
        exit()

    if args.tag:
        output_dir=f"studies/{args.study}/output_{args.tag}"
    else:
        output_dir=f"studies/{args.study}/output"

    os.makedirs(output_dir, exist_ok=True)
    logging.basicConfig(
        filename=f"{output_dir}/run.log",
        filemode="w",
        format="%(levelname)s [%(asctime)s]: %(message)s",
        datefmt="%m-%d-%Y %H:%M:%S %p",
        level="DEBUG"
    )

    if not args.no_make:
        print(f"make study={args.study} clean")
        Popen(f"make study={args.study} clean".split(), stdout=PIPE).wait()
        print(f"make study={args.study}")
        stdout, stderr = Popen(f"make study={args.study}".split(), stdout=PIPE, stderr=PIPE).communicate()
        if stderr:
            raise RuntimeError(
                f"make study={args.study} failed with the following error\n\n"
                + stderr.decode("utf-8")
            )

    if args.filter:
        filter_re = re.compile(args.filter)
        samples = [sample for sample in samples if filter_re.match(sample)]
        if not samples:
            print(f"Filter '{args.filter}' did not match any datasets")
            exit()
        print(f"Filter '{args.filter}' matched {len(samples)} datasets:")
        print("\n".join(samples))
        resp = ""
        while resp != "y" and resp != "n":
            resp = input("Run analysis jobs for these samples? y/n: ").lower()
        if resp == "n":
            exit()

    print(f"Orchestrating {len(samples)} jobs...")
    orchestrator = VBSOrchestrator(
        output_dir,
        args.output_ttree,
        f"./bin/{args.study}", 
        samples, 
        "data/xsecs.json",
        variation=args.var,
        n_workers=args.n_workers
    )
    orchestrator.run()
