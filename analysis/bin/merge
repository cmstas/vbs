#!/usr/bin/env python3
# -*- coding: utf-8 -*
import os
import glob
import argparse
from tqdm import tqdm
from multiprocessing import Pool
from subprocess import Popen, PIPE
from utils.cutflow import Cutflow, CutflowCollection

BKG_SAMPLE_MAP = {
    "SingleTop": {
        "2016preVFP": ["ST*"],
        "2016postVFP": ["ST*"],
        "2017": ["ST*"],
        "2018": ["ST*"],
    },
    "TTbar1L": {
        "2016preVFP": ["TTToSemiLep*"],
        "2016postVFP": ["TTToSemiLep*"],
        "2017": ["TTToSemiLep*"],
        "2018": ["TTToSemiLep*"],
    },
    "TTbar2L": {
        "2016preVFP": ["TTTo2L*"],
        "2016postVFP": ["TTTo2L*"],
        "2017": ["TTTo2L*"],
        "2018": ["TTTo2L*"],
    },
    "TTX": {
        "2016preVFP": ["ttH*", "TTW*", "TTZ*", "TTbb*"], # TODO: add TTToHadronic* here and below!
        "2016postVFP": ["ttH*", "TTW*", "TTZ*", "TTbb*"],
        "2017": ["ttH*", "TTW*", "TTZ*", "TTbb*"],
        "2018": ["ttH*", "TTW*", "TTZ*", "TTbb*"],
    },
    "WJets": {
        "2016preVFP": ["WJets*"],
        "2016postVFP": ["WJets*"],
        "2017": ["WJets*"],
        "2018": ["WJets*"],
    },
    "Bosons": {
        "2016preVFP": ["SSWW", "WW*", "WZ*", "ZZ*"],
        "2016postVFP": ["SSWW", "WW*", "WZ*", "ZZ*"],
        "2017": ["SSWW", "WW*", "WZ*", "ZZ*"],
        "2018": ["SSWW", "WW*", "WZ*", "ZZ*"],
    },
    "VH": {
        "2016preVFP": ["*HToBB*", "VHToNonbb_M125"],
        "2016postVFP": ["*HToBB*", "VHToNonbb_M125"],
        "2017": ["*HToBB*", "VHToNonbb_M125"],
        "2018": ["*HToBB*", "VHToNonbb_M125"]
    }
}

SIG_SAMPLE_MAP = {
    "VBSWH_mkW": {
        "2016preVFP": ["VBSWH_mkW_Inclusive"],
        "2016postVFP": ["VBSWH_mkW_Inclusive"],
        "2017": ["VBSWH_mkW_Inclusive"],
        "2018": ["VBSWH_mkW_Inclusive"]
    }
}

DATA_SAMPLE_MAP = {
    "data": {
        "2016preVFP": ["*Run201*"],
        "2016postVFP": ["*Run201*"],
        "2017": ["*Run201*"],
        "2018": ["*Run201*"]
    }
}

def hadd_job(split_cmd):
    hadd = Popen(split_cmd, stdout=PIPE, stderr=PIPE)
    hadd.wait()

def merge(output_dir, sample_map, n_hadders=8):
    # Collect cutflows and stage merge jobs
    hadd_cmds = []
    merged_cutflows = {}
    for group_name, group_map in sample_map.items():
        groups = {}
        root_files_to_merge = []
        for year, sample_list in group_map.items():
            groups[year] = []
            for sample_name in sample_list:
                cflow_files = glob.glob(f"{output_dir}/{year}/{sample_name}_Cutflow.cflow")
                for cflow_file in cflow_files:
                    if group_name in merged_cutflows.keys():
                        merged_cutflows[group_name] += Cutflow.from_file(cflow_file)
                    else:
                        merged_cutflows[group_name] = Cutflow.from_file(cflow_file)
                root_files = glob.glob(f"{output_dir}/{year}/{sample_name}.root")
                for root_file in root_files:
                    root_files_to_merge.append(root_file)
                    groups[year].append(root_file.split("/")[-1].replace(".root", ""))
        # Stage merge (hadd) jobs
        output_file = f"{output_dir}/Run2/{group_name}.root"
        hadd_cmds.append(["hadd", output_file] + root_files_to_merge)
        if os.path.exists(output_file):
            os.remove(output_file)
        # Write list of files that were hadded
        with open(output_file.replace(".root", ".txt"), "w") as f_out:
            for year, group in groups.items():
                f_out.write("{0}:\n{1}\n\n".format(year, '\n'.join(sorted(group))))

    # Run hadd jobs
    with Pool(processes=n_hadders) as pool:
        list(tqdm(pool.imap(hadd_job, hadd_cmds), total=len(hadd_cmds), desc="Executing hadds"))

    if merged_cutflows:
        return CutflowCollection(merged_cutflows)
    else:
        return None

if __name__ == "__main__":
    cli = argparse.ArgumentParser(description="Run merge results from /bin/run")
    cli.add_argument(
        "study", type=str,
        help="Name of the study to run"
    )
    cli.add_argument(
        "--terminals", nargs="*",
        help="Names of terminal cuts in cutflow to write to CSV"
    )
    cli.add_argument(
        "--debug", action="store_true",
        help="Run in debug mode"
    )
    cli.add_argument(
        "--n_workers", type=int, default=8,
        help="Number of workers to run hadds"
    )
    args = cli.parse_args()
    output_dir = f"studies/{args.study}/output"
    os.makedirs(f"{output_dir}/Run2", exist_ok=True)

    # Get Cutflow objects for background samples
    cutflows = merge(output_dir, BKG_SAMPLE_MAP, n_hadders=args.n_workers)
    cutflows["TotalBkg"] = cutflows.sum()
    # Get Cutflow objects for signal samples
    cutflows += merge(output_dir, SIG_SAMPLE_MAP, n_hadders=args.n_workers)
    cutflows.reorder(["VH", "Bosons", "WJets", "SingleTop", "TTX", "TTbar1L", "TTbar2L", "TotalBkg", "VBSWH_mkW"])

    # Write mermaid
    for group_name, cutflow in cutflows.items():
        cutflow.write_mermaid(f"{output_dir}/Run2/{group_name}_cutflow.mmd")
    # Write CSV
    cutflows.rename({
        "Bosons": "VV/VVV/VBSWZ", "TTX": 
        "TTbar+X", "WJets": "W+Jets", 
        "VBSWH_mkW": "VBSWH (kW < 0)"
    })
    for terminal_cut_name in args.terminals if args.terminals else cutflows.terminal_cut_names:
        cutflows.write_csv(f"{output_dir}/Run2/cutflow_{terminal_cut_name}.csv", terminal_cut_name)

    # Get Cutflow objects for data samples
    merge(output_dir, DATA_SAMPLE_MAP, n_hadders=args.n_workers)
